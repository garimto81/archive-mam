"""
ATI ë©”íƒ€ë°ì´í„° ì¸ë±ì‹± Cloud Function
v4.0.0 - Vertex AI Vector Search + BigQuery

GCS Pub/Sub íŠ¸ë¦¬ê±°:
- ATIê°€ GCSì— JSON ì €ì¥ ì‹œ ìë™ ì‹¤í–‰
- BigQueryì— ë©”íƒ€ë°ì´í„° ì‚½ì…
- Vertex AI Embedding ìƒì„± (í–¥í›„ Vector Search ì¸ë±ì‹±)

Deployment:
    gcloud functions deploy index-ati-metadata \
        --runtime python311 \
        --trigger-bucket ati-metadata-prod \
        --entry-point process_ati_metadata \
        --region us-central1 \
        --set-env-vars GCP_PROJECT=gg-poker-prod
"""

import functions_framework
from google.cloud import storage, bigquery, aiplatform
from google.cloud.exceptions import GoogleCloudError
from vertexai.language_models import TextEmbeddingModel
import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
import traceback

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ATIMetadataProcessor:
    """ATI ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, project_id: str):
        self.project_id = project_id
        self.storage_client = storage.Client(project=project_id)
        self.bq_client = bigquery.Client(project=project_id)
        self.dataset_id = "poker_archive"
        self.table_id = "hands"

        # Vertex AI ì´ˆê¸°í™”
        aiplatform.init(project=project_id, location="us-central1")
        self.embedding_model = TextEmbeddingModel.from_pretrained("text-embedding-004")

    def validate_metadata(self, metadata: Dict[str, Any]) -> bool:
        """ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        required_fields = [
            "hand_id", "tournament_id", "timestamp",
            "description", "hero_name", "pot_bb", "video_url"
        ]

        for field in required_fields:
            if field not in metadata:
                print(f"Missing required field: {field}")
                return False

        # íƒ€ì… ê²€ì¦
        if not isinstance(metadata["pot_bb"], (int, float)):
            print(f"Invalid pot_bb type: {type(metadata['pot_bb'])}")
            return False

        if not metadata["video_url"].startswith("gs://"):
            print(f"Invalid video_url format: {metadata['video_url']}")
            return False

        return True

    def transform_to_bigquery_row(
        self,
        metadata: Dict[str, Any],
        gcs_path: str
    ) -> Dict[str, Any]:
        """ATI ë©”íƒ€ë°ì´í„° â†’ BigQuery í–‰ ë³€í™˜"""
        now = datetime.utcnow()

        # ê¸°ë³¸ í–‰ ìƒì„±
        row = {
            # í•„ìˆ˜ í•„ë“œ
            "hand_id": metadata["hand_id"],
            "tournament_id": metadata["tournament_id"],
            "timestamp": metadata["timestamp"],
            "description": metadata["description"],
            "hero_name": metadata["hero_name"],
            "pot_bb": float(metadata["pot_bb"]),
            "video_url": metadata["video_url"],

            # ì‹œìŠ¤í…œ í•„ë“œ
            "created_date": now.date().isoformat(),
            "created_at": now.isoformat() + "Z",
            "gcs_source_path": gcs_path,
        }

        # ì„ íƒ í•„ë“œ (ìˆìœ¼ë©´ ì¶”ê°€)
        optional_fields = [
            "hand_number", "duration_seconds", "villain_name",
            "hero_position", "villain_position", "hero_stack_bb",
            "villain_stack_bb", "street", "hero_action", "result",
            "hand_type", "video_start_time", "video_end_time",
            "thumbnail_url", "ati_version", "ati_confidence"
        ]

        for field in optional_fields:
            if field in metadata:
                # Float ë³€í™˜ì´ í•„ìš”í•œ í•„ë“œ
                if field in ["hero_stack_bb", "villain_stack_bb",
                            "video_start_time", "video_end_time", "ati_confidence"]:
                    row[field] = float(metadata[field]) if metadata[field] is not None else None
                else:
                    row[field] = metadata[field]

        # ARRAY í•„ë“œ
        if "action_sequence" in metadata:
            row["action_sequence"] = metadata["action_sequence"]

        if "tags" in metadata:
            row["tags"] = metadata["tags"]

        return row

    def insert_to_bigquery(self, row: Dict[str, Any]) -> bool:
        """BigQueryì— í–‰ ì‚½ì…"""
        table_ref = f"{self.project_id}.{self.dataset_id}.{self.table_id}"

        try:
            errors = self.bq_client.insert_rows_json(table_ref, [row])

            if errors:
                print(f"BigQuery insert errors: {errors}")
                return False

            print(f"âœ… BigQuery insert successful: {row['hand_id']}")
            return True

        except GoogleCloudError as e:
            print(f"BigQuery insert failed: {e}")
            print(traceback.format_exc())
            return False

    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """Vertex AIë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ (description)

        Returns:
            768ì°¨ì› ì„ë² ë”© ë²¡í„° ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        try:
            # TextEmbedding-004 ëª¨ë¸ ì‚¬ìš©
            embeddings = self.embedding_model.get_embeddings([text])

            if embeddings and len(embeddings) > 0:
                embedding_values = embeddings[0].values
                print(f"âœ… Embedding generated: {len(embedding_values)} dimensions")
                return embedding_values
            else:
                print("âŒ Embedding generation failed: empty response")
                return None

        except Exception as e:
            print(f"âŒ Embedding generation error: {e}")
            print(traceback.format_exc())
            return None

    def save_embedding_to_gcs(
        self,
        hand_id: str,
        embedding: List[float],
        bucket_name: str = "ati-metadata-prod"
    ) -> bool:
        """ì„ë² ë”©ì„ GCSì— JSONìœ¼ë¡œ ì €ì¥ (Vertex AI ì¸ë±ìŠ¤ ì—…ë¡œë“œìš©)

        Args:
            hand_id: í•¸ë“œ ID
            embedding: ì„ë² ë”© ë²¡í„°
            bucket_name: GCS ë²„í‚· ì´ë¦„

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # embeddings/ í´ë”ì— ì €ì¥
            embedding_data = {
                "id": hand_id,
                "embedding": embedding
            }

            bucket = self.storage_client.bucket(bucket_name)
            blob = bucket.blob(f"embeddings/{hand_id}.json")
            blob.upload_from_string(
                json.dumps(embedding_data),
                content_type="application/json"
            )

            print(f"âœ… Embedding saved to GCS: embeddings/{hand_id}.json")
            return True

        except Exception as e:
            print(f"âŒ Embedding save failed: {e}")
            print(traceback.format_exc())
            return False

    def process_gcs_file(self, bucket_name: str, file_name: str) -> bool:
        """GCS íŒŒì¼ ì²˜ë¦¬ ë©”ì¸ ë¡œì§"""
        gcs_path = f"gs://{bucket_name}/{file_name}"

        print(f"Processing file: {gcs_path}")

        try:
            # 1. GCSì—ì„œ JSON ì½ê¸°
            bucket = self.storage_client.bucket(bucket_name)
            blob = bucket.blob(file_name)

            if not blob.exists():
                print(f"File not found: {gcs_path}")
                return False

            content = blob.download_as_text()
            metadata = json.loads(content)

            print(f"Loaded JSON: {metadata.get('hand_id', 'unknown')}")

            # 2. ìŠ¤í‚¤ë§ˆ ê²€ì¦
            if not self.validate_metadata(metadata):
                print("Schema validation failed")
                return False

            print("âœ… Schema validation passed")

            # 3. BigQuery í–‰ ë³€í™˜
            bq_row = self.transform_to_bigquery_row(metadata, gcs_path)

            # 4. BigQuery ì‚½ì…
            success = self.insert_to_bigquery(bq_row)

            if not success:
                return False

            # 5. Vertex AI Embedding ìƒì„± ë° ì €ì¥
            embedding = self.generate_embedding(metadata["description"])

            if embedding:
                # GCSì— ì„ë² ë”© ì €ì¥ (Vertex AI ì¸ë±ìŠ¤ ì—…ë¡œë“œìš©)
                embedding_saved = self.save_embedding_to_gcs(
                    metadata["hand_id"],
                    embedding,
                    bucket_name
                )

                if not embedding_saved:
                    print("âš ï¸  Embedding save failed, but continuing...")
            else:
                print("âš ï¸  Embedding generation failed, but continuing...")

            print(f"âœ… Processing complete: {metadata['hand_id']}")
            return True

        except json.JSONDecodeError as e:
            print(f"Invalid JSON: {e}")
            return False

        except Exception as e:
            print(f"Unexpected error: {e}")
            print(traceback.format_exc())
            return False


@functions_framework.cloud_event
def process_ati_metadata(cloud_event):
    """
    Cloud Function Entry Point (GCS Pub/Sub íŠ¸ë¦¬ê±°)

    Args:
        cloud_event: CloudEvent with GCS object metadata

    Returns:
        None (ë¡œê·¸ë§Œ ì¶œë ¥)
    """
    import os

    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ í”„ë¡œì íŠ¸ ID ê°€ì ¸ì˜¤ê¸°
        project_id = os.environ.get("GCP_PROJECT", "gg-poker-prod")

        # ì´ë²¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
        data = cloud_event.data
        bucket_name = data["bucket"]
        file_name = data["name"]

        print("=" * 60)
        print(f"Cloud Function Triggered")
        print(f"Bucket: {bucket_name}")
        print(f"File: {file_name}")
        print("=" * 60)

        # embeddings/ í´ë” íŒŒì¼ ê±´ë„ˆë›°ê¸° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        if file_name.startswith('embeddings/'):
            print(f"Skipping embedding file: {file_name}")
            return

        # JSON íŒŒì¼ë§Œ ì²˜ë¦¬ (ë””ë ‰í† ë¦¬ë‚˜ ê¸°íƒ€ íŒŒì¼ ì œì™¸)
        if not file_name.endswith('.json'):
            print(f"Skipping non-JSON file: {file_name}")
            return

        # í”„ë¡œì„¸ì„œ ìƒì„± ë° ì‹¤í–‰
        processor = ATIMetadataProcessor(project_id)
        success = processor.process_gcs_file(bucket_name, file_name)

        if success:
            print("ğŸ‰ Processing completed successfully")
        else:
            print("âŒ Processing failed")
            # ì‹¤íŒ¨ ì‹œì—ë„ ì—ëŸ¬ë¥¼ raiseí•˜ì§€ ì•ŠìŒ (ì¬ì‹œë„ ë°©ì§€)
            # í•„ìš” ì‹œ Dead Letter Queue ì„¤ì •
    except Exception as e:
        print(f"FATAL ERROR in process_ati_metadata: {str(e)}")
        import traceback
        traceback.print_exc()


# ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© (Cloud Functions í™˜ê²½ ì™¸ë¶€)
if __name__ == "__main__":
    print("Local testing mode")
    print("Usage: python main.py")
    print("")
    print("To test:")
    print("  1. Set GCP_PROJECT environment variable")
    print("  2. Run: python -c 'from main import test_local; test_local()'")

    def test_local():
        """ë¡œì»¬ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
        import os

        project_id = os.environ.get("GCP_PROJECT", "gg-poker-prod")
        processor = ATIMetadataProcessor(project_id)

        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        test_bucket = "ati-metadata-prod"
        test_file = "test/ati_metadata_001.json"

        print(f"Testing with: gs://{test_bucket}/{test_file}")
        success = processor.process_gcs_file(test_bucket, test_file)

        if success:
            print("âœ… Test passed")
        else:
            print("âŒ Test failed")
