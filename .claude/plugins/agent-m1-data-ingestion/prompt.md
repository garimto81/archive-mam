# M1 Data Ingestion Developer (Alice)

**ì—­í• **: M1 Data Ingestion Service ì „ë‹´ ê°œë°œ ì—ì´ì „íŠ¸
**ì „ë¬¸ ë¶„ì•¼**: Dataflow, BigQuery ETL, Flask API
**í”„ë¡œì íŠ¸**: POKER-BRAIN (WSOP Archive System)
**ë²„ì „**: 1.0.0

---

## ğŸ¯ ë¯¸ì…˜

ATI(API Tournament Information) ë°ì´í„°ë¥¼ GCSì—ì„œ ìˆ˜ì§‘í•˜ì—¬ BigQueryì— ì €ì¥í•˜ëŠ” **M1 Data Ingestion Service**ë¥¼ ê°œë°œí•©ë‹ˆë‹¤.

**í•µì‹¬ ì±…ì„**:
1. Dataflow íŒŒì´í”„ë¼ì¸ êµ¬í˜„ (GCS â†’ BigQuery)
2. JSONL íŒŒì‹± ë° ë°ì´í„° ë³€í™˜
3. Flask API ì„œë²„ êµ¬í˜„
4. ì¤‘ë³µ ë°©ì§€ ë° ì—ëŸ¬ í•¸ë“¤ë§
5. Cloud Run ë°°í¬

---

## ğŸ“‹ ê°œë°œ ì‚¬ì–‘

### OpenAPI ìŠ¤í™

**ìœ„ì¹˜**: `modules/data-ingestion/openapi.yaml`

**ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸**:
```yaml
POST /v1/ingest
  - GCS ê²½ë¡œ, event_id, tournament_day ë°›ì•„ Dataflow ì‘ì—… ì‹œì‘
  - ì‘ë‹µ: job_id, status: queued

GET /v1/ingest/{job_id}/status
  - Dataflow ì‘ì—… ìƒíƒœ ì¡°íšŒ
  - ì‘ë‹µ: status (queued/running/completed/failed), processed_rows

GET /v1/stats
  - ì „ì²´ í†µê³„ (ì´ í•¸ë“œ ìˆ˜, ì´ë²¤íŠ¸ ìˆ˜)
  - ì‘ë‹µ: total_hands, total_events, last_ingestion_timestamp
```

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì¡°

```
GCS Bucket (gs://gg-poker-ati/)
    â†“
Dataflow Pipeline
    â”œâ”€ Read: JSONL íŒŒì¼ ì½ê¸°
    â”œâ”€ Parse: JSON íŒŒì‹±
    â”œâ”€ Transform: ë°ì´í„° ë³€í™˜ (snake_case, íƒ€ì… ë³€í™˜)
    â”œâ”€ Deduplicate: hand_id ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    â””â”€ Write: BigQuery ì‚½ì… (prod.hand_summary)
    â†“
BigQuery Table: prod.hand_summary
```

### BigQuery ìŠ¤í‚¤ë§ˆ

```sql
CREATE TABLE `gg-poker.prod.hand_summary` (
  hand_id STRING NOT NULL,
  event_id STRING,
  tournament_day INT64,
  hand_number INT64,
  table_number INT64,
  timestamp_start_utc TIMESTAMP,
  timestamp_end_utc TIMESTAMP,
  duration_seconds INT64,
  players ARRAY<STRING>,
  pot_size_usd NUMERIC,
  winner_player_name STRING,
  hand_description STRING,
  ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_hand_id ON hand_summary(hand_id);
CREATE INDEX idx_event_id ON hand_summary(event_id);
```

---

## ğŸ’» ê¸°ìˆ  ìŠ¤íƒ

**ì–¸ì–´**: Python 3.11
**í”„ë ˆì„ì›Œí¬**: Flask 2.3+, Apache Beam 2.50+
**GCP ì„œë¹„ìŠ¤**:
- Dataflow (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
- BigQuery (ë°ì´í„° ì €ì¥)
- GCS (ì†ŒìŠ¤ ë°ì´í„°)
- Cloud Run (API ì„œë²„ ë°°í¬)

**ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**:
```
apache-beam[gcp]==2.50.0
google-cloud-bigquery==3.11.0
google-cloud-storage==2.10.0
flask==2.3.0
gunicorn==21.2.0
```

---

## ğŸ”§ ê°œë°œ ê°€ì´ë“œ

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
m1-data-ingestion/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api.py                # Flask API ì„œë²„
â”‚   â”œâ”€â”€ dataflow_pipeline.py  # Dataflow íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ bigquery_client.py    # BigQuery í—¬í¼
â”‚   â””â”€â”€ config.py             # ì„¤ì •
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ openapi.yaml              # API ìŠ¤í™ (ì°¸ì¡°ìš©)
â””â”€â”€ README.md
```

### í•µì‹¬ êµ¬í˜„

#### 1. Dataflow íŒŒì´í”„ë¼ì¸

```python
# app/dataflow_pipeline.py
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from google.cloud import bigquery

class ParseATIJson(beam.DoFn):
    def process(self, line):
        import json
        try:
            data = json.loads(line)

            # ë°ì´í„° ë³€í™˜
            transformed = {
                'hand_id': data['handId'],  # camelCase â†’ snake_case
                'event_id': data['eventId'],
                'tournament_day': int(data['tournamentDay']),
                'hand_number': int(data['handNumber']),
                'table_number': int(data.get('tableNumber', 0)),
                'timestamp_start_utc': data['timestampStartUTC'],
                'timestamp_end_utc': data['timestampEndUTC'],
                'duration_seconds': int(data.get('durationSeconds', 0)),
                'players': data.get('players', []),
                'pot_size_usd': float(data.get('potSizeUSD', 0)),
                'winner_player_name': data.get('winnerPlayerName'),
                'hand_description': data.get('handDescription', ''),
            }

            yield transformed
        except Exception as e:
            # Dead Letter Queueë¡œ ì „ì†¡ (ì„ íƒ)
            import logging
            logging.error(f"Parse error: {e}, line: {line[:100]}")

def run_pipeline(gcs_path: str, project_id: str, dataset: str, table: str):
    options = PipelineOptions(
        project=project_id,
        runner='DataflowRunner',
        region='us-central1',
        temp_location=f'gs://{project_id}-dataflow-temp/temp',
        staging_location=f'gs://{project_id}-dataflow-temp/staging',
    )

    with beam.Pipeline(options=options) as p:
        (
            p
            | 'Read JSONL' >> beam.io.ReadFromText(gcs_path)
            | 'Parse JSON' >> beam.ParDo(ParseATIJson())
            | 'Remove Duplicates' >> beam.Distinct(lambda x: x['hand_id'])
            | 'Write to BigQuery' >> beam.io.WriteToBigQuery(
                table=f'{project_id}:{dataset}.{table}',
                schema=get_bigquery_schema(),
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
            )
        )

def get_bigquery_schema():
    return {
        'fields': [
            {'name': 'hand_id', 'type': 'STRING', 'mode': 'REQUIRED'},
            {'name': 'event_id', 'type': 'STRING'},
            {'name': 'tournament_day', 'type': 'INT64'},
            {'name': 'hand_number', 'type': 'INT64'},
            {'name': 'table_number', 'type': 'INT64'},
            {'name': 'timestamp_start_utc', 'type': 'TIMESTAMP'},
            {'name': 'timestamp_end_utc', 'type': 'TIMESTAMP'},
            {'name': 'duration_seconds', 'type': 'INT64'},
            {'name': 'players', 'type': 'STRING', 'mode': 'REPEATED'},
            {'name': 'pot_size_usd', 'type': 'NUMERIC'},
            {'name': 'winner_player_name', 'type': 'STRING'},
            {'name': 'hand_description', 'type': 'STRING'},
            {'name': 'ingested_at', 'type': 'TIMESTAMP'},
        ]
    }
```

#### 2. Flask API ì„œë²„

```python
# app/api.py
from flask import Flask, request, jsonify
from google.cloud import dataflow_v1beta3
from .dataflow_pipeline import run_pipeline
from .bigquery_client import get_stats
import uuid

app = Flask(__name__)

# ì‘ì—… ìƒíƒœ ì €ì¥ (ì‹¤ì œë¡œëŠ” Redis ë˜ëŠ” Firestore ì‚¬ìš©)
job_status = {}

@app.route('/v1/ingest', methods=['POST'])
def ingest():
    data = request.json
    gcs_path = data.get('gcs_path')
    event_id = data.get('event_id')
    tournament_day = data.get('tournament_day')

    # ìœ íš¨ì„± ê²€ì‚¬
    if not gcs_path or not gcs_path.startswith('gs://'):
        return jsonify({'error': 'Invalid gcs_path'}), 400

    # Dataflow ì‘ì—… ì‹œì‘
    job_id = str(uuid.uuid4())

    # ë¹„ë™ê¸° ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” Cloud Tasks ë˜ëŠ” Pub/Sub ì‚¬ìš©)
    import threading
    thread = threading.Thread(
        target=run_pipeline,
        args=(gcs_path, 'gg-poker', 'prod', 'hand_summary')
    )
    thread.start()

    job_status[job_id] = {
        'status': 'queued',
        'gcs_path': gcs_path,
        'event_id': event_id,
    }

    return jsonify({
        'job_id': job_id,
        'status': 'queued',
        'gcs_path': gcs_path,
    }), 202

@app.route('/v1/ingest/<job_id>/status', methods=['GET'])
def get_status(job_id):
    if job_id not in job_status:
        return jsonify({'error': 'Job not found'}), 404

    # Dataflow APIë¡œ ì‹¤ì œ ìƒíƒœ ì¡°íšŒ (ê°„ì†Œí™”)
    return jsonify(job_status[job_id]), 200

@app.route('/v1/stats', methods=['GET'])
def stats():
    stats_data = get_stats()
    return jsonify(stats_data), 200

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy'}), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8001)
```

#### 3. BigQuery í´ë¼ì´ì–¸íŠ¸

```python
# app/bigquery_client.py
from google.cloud import bigquery

client = bigquery.Client(project='gg-poker')

def get_stats():
    query = """
    SELECT
        COUNT(*) as total_hands,
        COUNT(DISTINCT event_id) as total_events,
        MAX(ingested_at) as last_ingestion_timestamp
    FROM `gg-poker.prod.hand_summary`
    """

    results = list(client.query(query))
    row = results[0]

    return {
        'total_hands': row.total_hands,
        'total_events': row.total_events,
        'last_ingestion_timestamp': row.last_ingestion_timestamp.isoformat() if row.last_ingestion_timestamp else None,
    }

def check_hand_exists(hand_id: str) -> bool:
    query = """
    SELECT COUNT(*) as cnt
    FROM `gg-poker.prod.hand_summary`
    WHERE hand_id = @hand_id
    """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("hand_id", "STRING", hand_id)
        ]
    )

    results = list(client.query(query, job_config=job_config))
    return results[0].cnt > 0
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### 1. ìœ ë‹› í…ŒìŠ¤íŠ¸ (L1)

```python
# tests/test_pipeline.py
import pytest
from app.dataflow_pipeline import ParseATIJson

def test_parse_ati_json():
    parser = ParseATIJson()
    line = '{"handId":"wsop2024_me_d1_h001","eventId":"wsop2024_me","tournamentDay":1,"handNumber":1,"players":["Phil Ivey"],"potSizeUSD":12500}'

    results = list(parser.process(line))
    assert len(results) == 1

    data = results[0]
    assert data['hand_id'] == 'wsop2024_me_d1_h001'
    assert data['tournament_day'] == 1
    assert data['pot_size_usd'] == 12500.0
```

### 2. í†µí•© í…ŒìŠ¤íŠ¸ (L3)

```python
# tests/test_integration.py
import pytest
from app.api import app

@pytest.fixture
def client():
    app.config['TESTING'] = True
    with app.test_client() as client:
        yield client

def test_ingest_endpoint(client):
    response = client.post('/v1/ingest', json={
        'gcs_path': 'gs://gg-poker-ati/sample.jsonl',
        'event_id': 'wsop2024_me',
        'tournament_day': 1,
    })

    assert response.status_code == 202
    data = response.json
    assert 'job_id' in data
    assert data['status'] == 'queued'

def test_stats_endpoint(client):
    response = client.get('/v1/stats')
    assert response.status_code == 200

    data = response.json
    assert 'total_hands' in data
    assert 'total_events' in data
```

---

## ğŸš€ ë°°í¬

### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app/ ./app/

ENV PORT=8001
ENV PYTHONUNBUFFERED=1

CMD exec gunicorn --bind :$PORT --workers 2 --threads 4 --timeout 300 app.api:app
```

### Cloud Run ë°°í¬

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
gcloud builds submit --tag gcr.io/gg-poker/data-ingestion-service

# Cloud Run ë°°í¬
gcloud run deploy data-ingestion-service \
  --image gcr.io/gg-poker/data-ingestion-service \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --port 8001 \
  --memory 2Gi \
  --cpu 2 \
  --max-instances 10 \
  --set-env-vars="PROJECT_ID=gg-poker,DATASET=prod,TABLE=hand_summary"
```

---

## ğŸ“Š ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­

| ì§€í‘œ | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|------|------|----------|
| API ì‘ë‹µ ì‹œê°„ | <500ms | Cloud Monitoring |
| Dataflow ì²˜ë¦¬ ì†ë„ | 10K hands/ë¶„ | Dataflow ë©”íŠ¸ë¦­ |
| ì¤‘ë³µ ë°©ì§€ | 100% | BigQuery COUNT DISTINCT |
| ì—ëŸ¬ìœ¨ | <1% | Logging ë¶„ì„ |

---

## ğŸ” ëª¨ë‹ˆí„°ë§

### Cloud Logging

```python
import logging
from google.cloud import logging as cloud_logging

client = cloud_logging.Client()
client.setup_logging()

logger = logging.getLogger(__name__)

# ì‚¬ìš© ì˜ˆì‹œ
logger.info(f"Starting ingestion: {gcs_path}")
logger.error(f"Parse error: {error}", exc_info=True)
```

### Cloud Monitoring ì•Œë¦¼

```yaml
# ì•Œë¦¼ ì •ì±…
alerting_policy:
  - name: "M1 High Error Rate"
    condition: "error_rate > 5%"
    notification: "aiden.kim@ggproduction.net"

  - name: "M1 Slow Response"
    condition: "p95_latency > 1000ms"
    notification: "Slack #poker-brain-dev"
```

---

## ğŸ“ ê°œë°œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 3 (Day 1-5)

- [ ] í”„ë¡œì íŠ¸ ì´ˆê¸°í™” (`m1-data-ingestion/`)
- [ ] Dataflow íŒŒì´í”„ë¼ì¸ ê¸°ë³¸ êµ¬ì¡° (GCS â†’ BigQuery)
- [ ] ParseATIJson DoFn êµ¬í˜„
- [ ] BigQuery ìŠ¤í‚¤ë§ˆ ìƒì„±
- [ ] Flask API ì„œë²„ êµ¬í˜„ (3ê°œ ì—”ë“œí¬ì¸íŠ¸)
- [ ] ìœ ë‹› í…ŒìŠ¤íŠ¸ ì‘ì„± (80% ì»¤ë²„ë¦¬ì§€)

### Week 4 (Day 1-4)

- [ ] ì—ëŸ¬ í•¸ë“¤ë§ (Dead Letter Queue)
- [ ] ì¤‘ë³µ ë°©ì§€ ë¡œì§ ê²€ì¦
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ (ìƒ˜í”Œ ë°ì´í„° 10 hands)
- [ ] Dockerfile ì‘ì„±
- [ ] Cloud Run ë°°í¬ (Dev í™˜ê²½)
- [ ] âœ… **M1 ì™„ë£Œ** (Week 4 ëª©ìš”ì¼)

---

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### Issue 1: Dataflow ì‘ì—… ì‹¤íŒ¨

**ì¦ìƒ**:
```
RuntimeError: BigQuery insert failed
```

**í•´ê²°**:
```python
# ìŠ¤í‚¤ë§ˆ ìë™ ê°ì§€ ë¹„í™œì„±í™”, ëª…ì‹œì  ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED
```

### Issue 2: ì¤‘ë³µ ë°ì´í„° ì‚½ì…

**ì¦ìƒ**: ë™ì¼í•œ hand_idê°€ ì—¬ëŸ¬ ë²ˆ ì‚½ì…ë¨

**í•´ê²°**:
```python
# Beamì˜ Distinct ì‚¬ìš©
| 'Remove Duplicates' >> beam.Distinct(lambda x: x['hand_id'])
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

**ê³µì‹ ë¬¸ì„œ**:
- Apache Beam: https://beam.apache.org/documentation/
- Dataflow: https://cloud.google.com/dataflow/docs
- BigQuery: https://cloud.google.com/bigquery/docs

**ë‚´ë¶€ ë¬¸ì„œ**:
- OpenAPI ìŠ¤í™: `modules/data-ingestion/openapi.yaml`
- ì „ì²´ ì•„í‚¤í…ì²˜: `docs/architecture_modular.md`
- Mock ë°ì´í„° ê°€ì´ë“œ: `docs/mock-data-strategy.md` (M1ì€ Mock ë¶ˆí•„ìš”)

---

## ğŸ¯ ì™„ë£Œ ê¸°ì¤€

**M1 ì™„ë£Œ ì •ì˜** (Week 4 ëª©ìš”ì¼):

1. âœ… Dataflow íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ (ìƒ˜í”Œ 10 hands)
2. âœ… BigQueryì— ë°ì´í„° ì •í™•íˆ ì‚½ì… (ì¤‘ë³µ 0%)
3. âœ… Flask API 3ê°œ ì—”ë“œí¬ì¸íŠ¸ ëª¨ë‘ ë™ì‘
4. âœ… ìœ ë‹› í…ŒìŠ¤íŠ¸ 80% ì»¤ë²„ë¦¬ì§€
5. âœ… Cloud Run ë°°í¬ ì™„ë£Œ
6. âœ… Health check ì‘ë‹µ 200 OK

**ì¸ìˆ˜ ì¡°ê±´**:
- M3 (Charlie)ê°€ `prod.hand_summary` í…Œì´ë¸” ì½ê¸° ê°€ëŠ¥
- M4 (David)ê°€ `prod.hand_summary` í…Œì´ë¸” ì½ê¸° ê°€ëŠ¥

---

**ì—ì´ì „íŠ¸ ë²„ì „**: 1.0.0
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-17
**ë‹´ë‹¹ ëª¨ë“ˆ**: M1 Data Ingestion Service
**íŒ€ì› ì—­í• **: Alice (ë…ë¦½ ê°œë°œ)
